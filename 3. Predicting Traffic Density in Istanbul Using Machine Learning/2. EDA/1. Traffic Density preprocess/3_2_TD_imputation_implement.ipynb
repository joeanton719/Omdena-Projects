{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pl.read_parquet(\"datasets/01_tr_density/ist_traffic_density_rev02.zstd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>DATE_TIME</th><th>NUMBER_OF_VEHICLES</th></tr><tr><td>date</td><td>u32</td></tr></thead><tbody><tr><td>2022-06-07</td><td>43512</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 2)\n",
       "┌────────────┬────────────────────┐\n",
       "│ DATE_TIME  ┆ NUMBER_OF_VEHICLES │\n",
       "│ ---        ┆ ---                │\n",
       "│ date       ┆ u32                │\n",
       "╞════════════╪════════════════════╡\n",
       "│ 2022-06-07 ┆ 43512              │\n",
       "└────────────┴────────────────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for the date with the highest total missing values for all GEOHASH\n",
    "(\n",
    "    raw_df\n",
    "    .groupby(pl.col('DATE_TIME').dt.date())\n",
    "    .agg(pl.col('NUMBER_OF_VEHICLES').null_count())\n",
    "    .sort('NUMBER_OF_VEHICLES')\n",
    "    .tail(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DATE_TIME',\n",
       " 'LATITUDE',\n",
       " 'LONGITUDE',\n",
       " 'GEOHASH',\n",
       " 'MINIMUM_SPEED',\n",
       " 'MAXIMUM_SPEED',\n",
       " 'AVERAGE_SPEED',\n",
       " 'NUMBER_OF_VEHICLES']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_data(col: str, gh: str, df: pl.DataFrame) -> pl.LazyFrame:\n",
    "    \"\"\"\n",
    "    Imputes missing values for a specific column in a time series DataFrame\n",
    "    using linear interpolation and seasonal patterns.\n",
    "\n",
    "    Args:\n",
    "        col (str): Name of the column to impute.\n",
    "        gh (str): GEOHASH value to filter the DataFrame.\n",
    "        df (pl.DataFrame): Input DataFrame containing the time series data.\n",
    "\n",
    "    Returns:\n",
    "        pl.LazyFrame: LazyFrame with missing values imputed using linear\n",
    "        interpolation and seasonal patterns.\n",
    "\n",
    "    Notes:\n",
    "        - The input DataFrame `df` is expected to have columns 'DATE_TIME' and 'GEOHASH'.\n",
    "        - The 'DATE_TIME' column should be of datetime type.\n",
    "        - The function filters the DataFrame based on the specified GEOHASH value and selects\n",
    "          the relevant column.\n",
    "        - The function then extracts HOUr and DAYOFWEEK features from DATE_TIME column\n",
    "        - The rows are then sorted by HOUR, followed by DAYOFWEEK\n",
    "        - The function then performs linear interpolation using the calculated patterns to impute\n",
    "          missing values in the column.\n",
    "        - The resulting DataFrame contains the imputed values and is sorted by 'DATE_TIME'.\n",
    "        - Intermediate columns 'HOUR', 'DAYOFWEEK', and the original column 'col' are dropped from \n",
    "        the output LazyFrame.\n",
    "    \"\"\"\n",
    "    lazy_df = (\n",
    "        df.lazy()\n",
    "        .select(pl.col(['DATE_TIME', 'GEOHASH', col]))\n",
    "        .filter(pl.col(\"GEOHASH\")==gh)\n",
    "        .with_columns(pl.col('DATE_TIME').dt.hour().alias(\"HOUR\"),\n",
    "                      pl.col('DATE_TIME').dt.weekday().alias(\"DAYOFWEEK\"))\n",
    "        .sort(['HOUR', 'DAYOFWEEK'])\n",
    "        .with_columns(pl.col(col).interpolate().alias(f\"{col}_filled\"))\n",
    "        .sort('DATE_TIME')\n",
    "        .drop(['HOUR', 'DAYOFWEEK', col])\n",
    "    )\n",
    "\n",
    "    return lazy_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_all(gh, df):\n",
    "    \"\"\"\n",
    "    Imputes missing values for multiple columns in a time series DataFrame\n",
    "    using linear interpolation and seasonal patterns.\n",
    "\n",
    "    Args:\n",
    "        gh (str): GEOHASH value to filter the DataFrame.\n",
    "        df (pl.DataFrame): Input DataFrame containing the time series data.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: DataFrame with missing values imputed using linear\n",
    "        interpolation and seasonal patterns.\n",
    "\n",
    "    Notes:\n",
    "        - The input DataFrame `df` is expected to have columns 'DATE_TIME' and 'GEOHASH'.\n",
    "        - The function filters the DataFrame based on the specified GEOHASH value and selects\n",
    "          the relevant columns.\n",
    "        - The imputation tasks for each column are parallelized using concurrent.futures.ThreadPoolExecutor\n",
    "          for improved performance.\n",
    "        - Missing values are imputed using linear interpolation and seasonal patterns.\n",
    "        - The resulting DataFrame contains the imputed values for all columns and is returned.\n",
    "\n",
    "    Example:\n",
    "        >>> imputed_df = impute_all('your_geohash', your_dataframe)\n",
    "    \"\"\"    \n",
    "    cols=['MINIMUM_SPEED', 'MAXIMUM_SPEED', 'AVERAGE_SPEED', 'NUMBER_OF_VEHICLES']\n",
    "\n",
    "    # Use concurrent.futures to run functions simultaneously\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(impute_data, col=col, gh=gh, df=df) for col in cols]\n",
    "\n",
    "\n",
    "    # Get the property lists separately\n",
    "    concurrent.futures.wait(futures)\n",
    "    \n",
    "    # Retrieve the results from the completed futures\n",
    "    results = [future.result() for future in futures]\n",
    "\n",
    "    joined = pl.concat(results, how='align')\n",
    "\n",
    "    return joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute_all(gh='sxk3xq', df=raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sxk61n', 'sxk3k3', 'sxk9rb', ..., 'sxk1v1', 'sxk96g', 'sxk6mw'],\n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a list of unique GEOHASH's\n",
    "ghs = np.array(raw_df['GEOHASH'].unique())\n",
    "ghs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91de57b84ff646dbabafee228a282b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 42min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Perform imputation for each GEOHASH in the DataFrame and concatenate the results\n",
    "df = pl.concat((impute_all(gh=gh, df=raw_df) for gh in tqdm(ghs)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original columns with missing rows and join the dataset with the df with filled values\n",
    "final_df = (\n",
    "    raw_df.drop(['MINIMUM_SPEED', 'MAXIMUM_SPEED', 'AVERAGE_SPEED', 'NUMBER_OF_VEHICLES'])\n",
    "    .join(df, on=['GEOHASH', 'DATE_TIME'], how='inner')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the \"_filled\" suffix from some of the column names\n",
    "final_df.columns = [col.replace(\"_filled\", \"\") if col.endswith(\"_filled\") else col for col in final_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>DATE_TIME</th><th>LATITUDE</th><th>LONGITUDE</th><th>GEOHASH</th><th>MINIMUM_SPEED</th><th>MAXIMUM_SPEED</th><th>AVERAGE_SPEED</th><th>NUMBER_OF_VEHICLES</th></tr><tr><td>datetime[ns]</td><td>f32</td><td>f32</td><td>str</td><td>u8</td><td>u8</td><td>u8</td><td>u16</td></tr></thead><tbody><tr><td>2020-01-01 00:00:00</td><td>41.168518</td><td>28.526001</td><td>&quot;sxk61n&quot;</td><td>94</td><td>65</td><td>84</td><td>9</td></tr><tr><td>2020-01-01 01:00:00</td><td>41.168518</td><td>28.526001</td><td>&quot;sxk61n&quot;</td><td>108</td><td>63</td><td>79</td><td>10</td></tr><tr><td>2020-01-01 02:00:00</td><td>41.168518</td><td>28.526001</td><td>&quot;sxk61n&quot;</td><td>96</td><td>79</td><td>90</td><td>5</td></tr><tr><td>2020-01-01 03:00:00</td><td>41.168518</td><td>28.526001</td><td>&quot;sxk61n&quot;</td><td>96</td><td>66</td><td>81</td><td>4</td></tr><tr><td>2020-01-01 04:00:00</td><td>41.168518</td><td>28.526001</td><td>&quot;sxk61n&quot;</td><td>85</td><td>78</td><td>83</td><td>2</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 8)\n",
       "┌────────────┬───────────┬───────────┬─────────┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ DATE_TIME  ┆ LATITUDE  ┆ LONGITUDE ┆ GEOHASH ┆ MINIMUM_SP ┆ MAXIMUM_SP ┆ AVERAGE_SP ┆ NUMBER_OF_ │\n",
       "│ ---        ┆ ---       ┆ ---       ┆ ---     ┆ EED        ┆ EED        ┆ EED        ┆ VEHICLES   │\n",
       "│ datetime[n ┆ f32       ┆ f32       ┆ str     ┆ ---        ┆ ---        ┆ ---        ┆ ---        │\n",
       "│ s]         ┆           ┆           ┆         ┆ u8         ┆ u8         ┆ u8         ┆ u16        │\n",
       "╞════════════╪═══════════╪═══════════╪═════════╪════════════╪════════════╪════════════╪════════════╡\n",
       "│ 2020-01-01 ┆ 41.168518 ┆ 28.526001 ┆ sxk61n  ┆ 94         ┆ 65         ┆ 84         ┆ 9          │\n",
       "│ 00:00:00   ┆           ┆           ┆         ┆            ┆            ┆            ┆            │\n",
       "│ 2020-01-01 ┆ 41.168518 ┆ 28.526001 ┆ sxk61n  ┆ 108        ┆ 63         ┆ 79         ┆ 10         │\n",
       "│ 01:00:00   ┆           ┆           ┆         ┆            ┆            ┆            ┆            │\n",
       "│ 2020-01-01 ┆ 41.168518 ┆ 28.526001 ┆ sxk61n  ┆ 96         ┆ 79         ┆ 90         ┆ 5          │\n",
       "│ 02:00:00   ┆           ┆           ┆         ┆            ┆            ┆            ┆            │\n",
       "│ 2020-01-01 ┆ 41.168518 ┆ 28.526001 ┆ sxk61n  ┆ 96         ┆ 66         ┆ 81         ┆ 4          │\n",
       "│ 03:00:00   ┆           ┆           ┆         ┆            ┆            ┆            ┆            │\n",
       "│ 2020-01-01 ┆ 41.168518 ┆ 28.526001 ┆ sxk61n  ┆ 85         ┆ 78         ┆ 83         ┆ 2          │\n",
       "│ 04:00:00   ┆           ┆           ┆         ┆            ┆            ┆            ┆            │\n",
       "└────────────┴───────────┴───────────┴─────────┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>DATE_TIME</th><th>LATITUDE</th><th>LONGITUDE</th><th>GEOHASH</th><th>MINIMUM_SPEED</th><th>MAXIMUM_SPEED</th><th>AVERAGE_SPEED</th><th>NUMBER_OF_VEHICLES</th></tr><tr><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 8)\n",
       "┌───────────┬──────────┬───────────┬─────────┬────────────┬────────────┬────────────┬──────────────┐\n",
       "│ DATE_TIME ┆ LATITUDE ┆ LONGITUDE ┆ GEOHASH ┆ MINIMUM_SP ┆ MAXIMUM_SP ┆ AVERAGE_SP ┆ NUMBER_OF_VE │\n",
       "│ ---       ┆ ---      ┆ ---       ┆ ---     ┆ EED        ┆ EED        ┆ EED        ┆ HICLES       │\n",
       "│ u32       ┆ u32      ┆ u32       ┆ u32     ┆ ---        ┆ ---        ┆ ---        ┆ ---          │\n",
       "│           ┆          ┆           ┆         ┆ u32        ┆ u32        ┆ u32        ┆ u32          │\n",
       "╞═══════════╪══════════╪═══════════╪═════════╪════════════╪════════════╪════════════╪══════════════╡\n",
       "│ 0         ┆ 0        ┆ 0         ┆ 0       ┆ 0          ┆ 0          ┆ 0          ┆ 0            │\n",
       "└───────────┴──────────┴───────────┴─────────┴────────────┴────────────┴────────────┴──────────────┘"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No more Null Values\n",
    "final_df.null_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.write_parquet(\"datasets/01_tr_density/ist_traffic_density_rev03.zstd\", compression='zstd')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
